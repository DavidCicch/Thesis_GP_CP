import sys
import copy 
from GP_CP_models.bayesianmodels_new import BayesianModel, GibbsState, ArrayTree
import GP_CP_models.Poisson.Poisson_Process as Poisson_Process

from GP_CP_models.GP_CP.meanfunctions import Zero
from GP_CP_models.GP_CP.likelihoods import AbstractLikelihood, Gaussian, RepeatedObsLikelihood

from GP_CP_models.GP_CP.gputil_new import sample_prior, sample_predictive, update_gaussian_process, update_gaussian_process_cov_params, update_gaussian_process_mean_params, update_gaussian_process_obs_params
from GP_CP_models.GP_CP.gputil_new import update_gaussian_process_cov_params_num_hyper, update_gaussian_process_cov_params_lv_hyper, update_gaussian_process_hyper_params
from jax import Array
from jaxtyping import Float
from jax.random import PRNGKeyArray as PRNGKey
from typing import Callable, Union, Dict, Any, Optional, Iterable, Mapping
# ArrayTree = Union[Array, Iterable["ArrayTree"], Mapping[Any, "ArrayTree"]]
from jax.tree_util import tree_flatten, tree_unflatten
from jax.flatten_util import ravel_pytree

from distrax._src.distributions.distribution import Distribution
from distrax._src.bijectors.bijector import Bijector

import jax
import distrax as dx
import jaxkern as jk
import jax.numpy as jnp
import jax.random as jrnd
from blackjax import elliptical_slice, rmh
from blackjax.smc.tempered import sequential_vmap


jitter = 1e-6


class FullGPModel(BayesianModel):

    def __init__(self, X, y: Optional[Array]=None,
                 cov_fn: Optional[Callable]=None,
                 mean_fn: Callable = None,
                 priors: Dict = None,
                 duplicate_input=False):
        if jnp.ndim(X) == 1:
            X = X[:, jnp.newaxis]   
        if cov_fn is None:
            cov_fn = jk.RBF()     
        # Validate arguments
        if y is not None and X.shape[0] > len(y):
            raise ValueError(
                f'X and y should have the same leading dimension, '
                f'but X has shape {X.shape} and y has shape {y.shape}.',
                f'Use the `FullLatentGPModelRepeatedObs` model for repeated inputs.')
        self.X, self.y = X, y        
        self.n = self.X.shape[0]        
        if mean_fn is None:
            mean_fn = Zero()
        self.mean_fn = mean_fn
        self.cov_fn = cov_fn
        self.param_priors = priors

    #
    def predict_f(self, key: PRNGKey, x_pred: ArrayTree):
        raise NotImplementedError

    #
    def predict_y(self, key: PRNGKey, x_pred: ArrayTree):
        raise NotImplementedError

    #
    def loglikelihood_fn(self) -> Callable:
        raise NotImplementedError
    
    #
    def plot_priors(self, axes=None):
        raise NotImplementedError

    #
    def inference(self, key: PRNGKey, mode='gibbs-in-smc', sampling_parameters: Dict = None):
        if not hasattr(self, 'y'):
            raise ValueError(f'Cannot perform inference on a GP model without',
                             f'providing observed responses y.')
        return super().inference(key, mode, sampling_parameters)

    #
    
#

       
class FullLatentGPModel(FullGPModel):

    """The latent Gaussian process model.

    The latent Gaussian process model consists of observations (y), generated by
    an observation model that takes the latent Gaussian process (f) and optional
    hyperparameters (phi) as input. The latent GP itself is parametrized by a
    mean function (mu) and a covariance function (cov). These can have optional
    hyperparameters (psi) and (theta).

    The generative model is given by:

    .. math::
        psi     &\sim p(\psi)\\
        theta   &\sim p(\theta) \\
        phi     &\sim p(\phi) \\
        f       &\sim GP(mu, cov) \\
        y       &\sim p(y \mid T(f), phi)

    Here, the scalar parameters are sampled using Gaussian random walk MCMC, 
    while the latent function f (or rather its evaluations) is sampled using
    Elliptical Slice Sampling.

    """

    def __init__(self, X, y: Optional[Array] = None,
                 cov_fn: Optional[Callable] = None,
                 mean_fn: Optional[Callable] = None,
                 priors: Dict = None,
                 likelihood: AbstractLikelihood = None,
                 **kwargs):
        if likelihood is None:
            self.likelihood = Gaussian()
        else:
            self.likelihood = likelihood

        super().__init__(X, y, cov_fn, mean_fn, priors, **kwargs)      

    #
    def init_fn(self, key, num_particles=1):
        """Initialization of the Gibbs state.

        The initial state is determined by sampling all variables from their
        priors, and then constructing one sample for (f) using these. All
        variables together are stored in a dict() in the GibbsState object.

        When num_particles > 1, each dict element contains num_particles random
        initial values.

        Args:
            key:
                The jax.random.PRNGKey
            num_particles: int
                The number of parallel particles for sequential Monte Carlo
        Returns:
            GibbsState

        """

        initial_state = super().init_fn(key, num_particles)
        initial_position = initial_state.position

        mean_params = initial_position.get('mean', {})
        cov_params = initial_position.get('kernel', {})
        mean_param_in_axes = jax.tree_map(lambda l: 0, mean_params)
        cov_param_in_axes = jax.tree_map(lambda l: 0, cov_params)

        if num_particles > 1:
            keys = jrnd.split(key, num_particles)                
            sample_fun = lambda key_, mean_params_, cov_params_: sample_prior(key=key_, 
                                                                                mean_params=mean_params_,
                                                                                cov_params=cov_params_,
                                                                                mean_fn=self.mean_fn,
                                                                                cov_fn=self.cov_fn, 
                                                                                x=self.X)
            initial_position['f'] = jax.vmap(sample_fun,
                                             in_axes=(0,
                                                      mean_param_in_axes,
                                                      cov_param_in_axes))(keys, mean_params, cov_params)
        else:
            key, subkey = jrnd.split(key)
            initial_position['f'] = sample_prior(subkey, 
                                                 self.mean_fn, 
                                                 self.cov_fn, 
                                                 mean_params, 
                                                 cov_params, 
                                                 self.X)

        return GibbsState(initial_position)

        #

    def gibbs_fn(self, key: PRNGKey, state: GibbsState, temperature: Float= 1.0, **mcmc_parameters):
        """The Gibbs MCMC kernel.

        The Gibbs kernel step function takes a state and returns a new state. In
        the latent GP model, the latent GP (f) is first updated, then the
        parameters of the mean (psi) and covariance function (theta), and lastly
        the parameters of the observation model (phi).

        Args:
            key:
                The jax.random.PRNGKey
            state: GibbsState
                The current state in the MCMC sampler
            temperature: Float
                The likeihood temperature, \beta in p_\beta(x | y) \propto p(x) p(y | x)^\beta
            mcmc_parameters: Dict
                A dictionary with optional settings for the MCMC-within-Gibbs 
                steps. TODO
        Returns:
            GibbsState

        """
        position = state.position.copy()

        # Sample the latent GP using:   
        # p(f | theta, psi, y) \propto p(y | f, phi) p(f | psi, theta)

        likelihood_params = position.get('likelihood', {})
        mean_params = position.get('mean', {}) 
        cov_params = position.get('kernel', {}) 

        loglikelihood_fn_ = lambda f_: temperature * jnp.sum(self.likelihood.log_prob(params=likelihood_params, f=f_, y=self.y))

        key, subkey = jrnd.split(key)
        position['f'], f_info = update_gaussian_process(subkey,
                                                        position['f'],
                                                        loglikelihood_fn_,
                                                        self.X,
                                                        mean_fn=self.mean_fn,
                                                        cov_fn=self.cov_fn,
                                                        mean_params=mean_params,
                                                        cov_params=cov_params)

        if len(mean_params):
            # Sample parameters of the mean function using: 
            # p(psi | f, theta) \propto p(f | psi, theta)p(psi)      

            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_gaussian_process_mean_params(subkey, self.X,
                                       position['f'],
                                       mean_fn=self.mean_fn,
                                       cov_fn=self.cov_fn,
                                       mean_params=mean_params,
                                       cov_params=cov_params,
                                       hyperpriors=self.param_priors['mean'])
            position['mean'] = sub_state
            
        #

        if len(cov_params):
            # Sample parameters of the kernel function using: 
            # p(theta | f, psi) \propto p(f | psi, theta)p(theta)
            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_gaussian_process_cov_params(subkey, self.X,
                                       position['f'],
                                       mean_fn=self.mean_fn,
                                       cov_fn=self.cov_fn,
                                       mean_params=mean_params,
                                       cov_params=cov_params,
                                       hyperpriors=self.param_priors['kernel'])
            position['kernel'] = sub_state
        #

        if len(likelihood_params):
            # Sample parameters of the likelihood using: 
            # p(\phi | y, f) \propto p(y | f, phi)p(phi)

            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_gaussian_process_obs_params(subkey, self.y,
                                       position['f'],
                                       temperature=temperature,
                                       likelihood=self.likelihood,
                                       obs_params=likelihood_params,
                                       hyperpriors=self.param_priors['likelihood'])
            position['likelihood'] = sub_state
        
        #
        return GibbsState(position=position), None  # We return None to satisfy SMC; this needs to be filled with acceptance information

    #
    def loglikelihood_fn(self) -> Callable:
        """Returns the log-likelihood function for the model given a state.

        Args:
            None

        Returns:
            A function that computes the log-likelihood of the model given a
            state.
        """

        def loglikelihood_fn_(state: GibbsState) -> Float:
            position = getattr(state, 'position', state)
            phi = position.get('likelihood', {})
            f = position['f']
            log_pdf = jnp.sum(self.likelihood.log_prob(params=phi, f=f, y=self.y))
            return log_pdf

        #
        return loglikelihood_fn_

    #
    def logprior_fn(self) -> Callable:
        """Returns the log-prior function for the model given a state.

        Args:
            None
        Returns:
            A function that computes the log-prior of the model given a state.

        # todo: add 2D vmap

        """

        def logprior_fn_(state: GibbsState) -> Float:
            # to work in both Blackjax' MCMC and SMC environments
            position = getattr(state, 'position', state) 
            logprob = 0
            for component, params in self.param_priors.items():
                for param, dist in params.items():
                    logprob += jnp.sum(dist.log_prob(position[param]))
            psi = {param: position[param] for param in self.param_priors['mean']} if 'mean' in self.param_priors else {}
            theta = {param: position[param] for param in
                     self.param_priors['kernel']} if 'kernel' in self.param_priors else {}
            mean = self.mean_fn.mean(params=psi, x=self.X).flatten()
            cov = self.cov_fn.cross_covariance(params=theta,
                                               x=self.X,
                                               y=self.X) + jitter * jnp.eye(self.n)
            f = position['f']
            if jnp.ndim(f) == 1:
                logprob += dx.MultivariateNormalFullCovariance(mean, cov).log_prob(f)
            elif jnp.ndim(f) == 3:
                log_pdf += jnp.sum(jax.vmap(jax.vmap(dx.MultivariateNormalFullCovariance(mean, cov).log_prob, in_axes=1), in_axes=1)(f))
            else:
                raise NotImplementedError(f'Expected f to be of size (n,) or (n, nu, d),',
                                          f'but size {f.shape} was provided.')
            return logprob

        #
        return logprior_fn_

    #
    def predict_f(self, key: PRNGKey, x_pred: ArrayTree):
        """Samples from the posterior predictive of the latent f

        Args:
            key: PRNGKey
            x_pred: Array
                The test locatons
        Returns:
            Returns samples from the posterior predictive distribution:

            f* \sim p(f* | f, X, y x*) = \int p(f* | x*, f) p(f | X, y) df

        """
        if jnp.ndim(x_pred) == 1:
            x_pred = x_pred[:, jnp.newaxis]

        samples = self.get_monte_carlo_samples()
        if samples is None:
            raise AssertionError(
                f'The posterior predictive distribution can only be called after training.')

        num_particles = samples['f'].shape[0]
        key_samples = jrnd.split(key, num_particles)

        mean_params = samples.get('mean', {})
        cov_params = samples.get('kernel', {})
        mean_param_in_axes = jax.tree_map(lambda l: 0, mean_params)
        cov_param_in_axes = jax.tree_map(lambda l: 0, cov_params)

        sample_fun = lambda key, mean_params, cov_params, target: sample_predictive(key, 
                                                                            mean_params=mean_params, 
                                                                            cov_params=cov_params, 
                                                                            mean_fn=self.mean_fn,
                                                                            cov_fn=self.cov_fn, 
                                                                            x=self.X, 
                                                                            z=x_pred, 
                                                                            target=target)
        keys = jrnd.split(key, num_particles)
        target_pred = jax.vmap(jax.jit(sample_fun), 
                        in_axes=(0, 
                                 mean_param_in_axes, 
                                 cov_param_in_axes, 
                                 0))(keys, 
                                    mean_params, 
                                    cov_params, 
                                    samples['f'])

        return target_pred
    
    def predict_f_particle(self, key, x_pred, particles, num_subsample=-1):
        """Predict the latent f on unseen pointsand

        This function takes the approximated posterior (either by MCMC or SMC)
        and predicts new latent function evaluations f^*.

        Args:
            key: PRNGKey
            x_pred: x^*; the queried locations.
            num_subsample: By default, we return one predictive sample for each
            posterior sample. While accurate, this can be memory-intensive; this
            parameter can be used to thin the MC output to every n-th sample.

        Returns:
            f_samples: An array of samples of f^* from p(f^* | x^*, x, y)


        todo:
        - predict using either SMC or MCMC output
        - predict from prior if desired
        """
        if jnp.ndim(x_pred) == 1:
            x_pred = x_pred[:, jnp.newaxis]

        samples = copy.deepcopy(particles)
        flat_particles, _ = tree_flatten(samples)
        num_particles = flat_particles[0].shape[0]
        key_samples = jrnd.split(key, num_particles)

        mean_params = samples.get('mean', {})
        cov_params = samples.get('kernel', {})
        mean_params_in_axes = jax.tree_map(lambda l: 0, mean_params)
        cov_param_in_axes = jax.tree_map(lambda l: 0, cov_params)
        sample_fun = lambda key, mean_params_, cov_params_, target: sample_predictive(key,
                                                                            mean_params=mean_params_,
                                                                            cov_params=cov_params_,
                                                                            mean_fn=self.mean_fn,
                                                                            cov_fn=self.cov_fn,
                                                                            x=self.X,
                                                                            z=x_pred,
                                                                            target=target
                                                                            )
        
        keys = jrnd.split(key, num_particles)
        
        key_subset1 = keys[:num_particles//2]
        key_subset2 = keys[num_particles//2:num_particles]
        key_subset_list = jnp.array([key_subset1, key_subset2])
        # print(key_subset_list.shape)
        if isinstance(cov_params, dict):
            if 'num' in cov_params.keys():            
                cov_params['num'] = jnp.append(cov_params['num'], jnp.full((cov_params['num'].shape[0], 1), jnp.nan), axis=1)
        else:
             for i, param in enumerate(cov_params):
                  if 'num' in cov_params[i].keys():            
                    cov_params[i]['num'] = jnp.append(cov_params[i]['num'], jnp.full((cov_params[i]['num'].shape[0], 1), jnp.nan), axis=1)
        
        cov_params_subset1 = jax.tree_map(lambda l: l[:num_particles//2], cov_params)
        cov_params_subset2 = jax.tree_map(lambda l: l[num_particles//2:num_particles], cov_params)
        cov_params_subset1_flat, subset1_tree_def = jax.tree_util.tree_flatten(cov_params_subset1)
        cov_params_subset2_flat, subset2_tree_def = jax.tree_util.tree_flatten(cov_params_subset2)
        
        
        cov_params_subset_array1 = jnp.zeros((1, cov_params_subset1_flat[0].shape[0], cov_params_subset1_flat[0].shape[1]))
        cov_params_subset_array2 = jnp.zeros((1, cov_params_subset2_flat[0].shape[0], cov_params_subset1_flat[0].shape[1]))
        for arr in cov_params_subset1_flat:
             cov_params_subset_array1 = jnp.concatenate((cov_params_subset_array1, jnp.expand_dims(arr, 0)))
            
        for arr in cov_params_subset2_flat:
             cov_params_subset_array2 = jnp.concatenate((cov_params_subset_array2, jnp.expand_dims(arr, 0)))

        cov_params_subset_array1_new = cov_params_subset_array1[1:, :, :]
        cov_params_subset_array2_new = cov_params_subset_array2[1:, :, :]

        cov_params_subset_array = jnp.array([cov_params_subset_array1_new, cov_params_subset_array2_new])

        lik_samples1 = samples['likelihood']['obs_noise'][:num_particles//2]
        lik_samples2 = samples['likelihood']['obs_noise'][num_particles//2:num_particles]
        lik_subset_list = jnp.array([lik_samples1, lik_samples2])

        f_samples1 = samples['f'][:num_particles//2]
        f_samples2 = samples['f'][num_particles//2:num_particles]
        f_samples_list = jnp.array([f_samples1, f_samples2])


       
        def vmap_setup(key, m, cov, obs):
            new_cov = [x for x in cov]
            dict_cov = jax.tree_util.tree_unflatten(subset1_tree_def, new_cov)
            return jax.vmap(jax.jit(sample_fun), 
                              in_axes=(0, {k: 0 for k in mean_params}, cov_param_in_axes, 0))(key, m, dict_cov, obs)
        
        target_pred = jax.pmap(vmap_setup,
                                in_axes=(0,
                                        None,
                                        0,
                                        0), 
                                        devices = jax.devices())(key_subset_list,
                                                                    mean_params,
                                                                    cov_params_subset_array,
                                                                    f_samples_list)
                

        return jnp.reshape(target_pred, (-1, len(x_pred)))

    #
    def forward(self, key, params, f):
        """Sample from the likelihood, given likelihood parameters and latent f.

        """
        return self.likelihood.likelihood(params, f).sample(seed=key)

    #
    def predict_y(self, key: PRNGKey, x_pred: Array, particles = None):
        """Samples from the posterior predictive distribution

        Args:
            key: PRNGKey
            x_pred: Array
                The test locatons
        Returns:
            Returns samples from the posterior predictive distribution:

            y* \sim p(y* | X, y x*) = \int p(y* | f*)p(f* | f)p(f | X, y) df

        """

        if particles is not None:
            samples = particles
        else:
            samples = self.get_monte_carlo_samples()

        if jnp.ndim(x_pred) == 1:
            x_pred = x_pred[:, jnp.newaxis]
        
        if samples is None:
            raise AssertionError(
                f'The posterior predictive distribution can only be called after training.')
        
        key, key_f, key_y = jrnd.split(key, 3)
        if particles is not None:
            f_pred = self.predict_f_particle(key_f, x_pred, particles)
        else:
            f_pred = self.predict_f(key_f, x_pred)       
        num_particles = samples['f'].shape[0]
        keys_y = jrnd.split(key_y, num_particles)
        likelihood_params = samples.get('likelihood', {}) #{param: samples[param] for param in self.param_priors['likelihood']}
        obs_param_in_axes = jax.tree_map(lambda l: 0, likelihood_params)
        y_pred = jax.vmap(jax.jit(self.forward), 
                          in_axes=(0, 
                                   obs_param_in_axes, 
                                   0))(keys_y, 
                                    likelihood_params, 
                                    f_pred)
        return y_pred

    #
#

class FullLatentGPModelRepeatedObs(FullLatentGPModel):
    """An implementation of the full latent GP model that supports repeated 
    observations at a single input location. This class mostly inherits the 
    FullLatentGPModel, but identifies unique input locations and ensures these 
    are duplicated at likelihood evaluations.

    """

    def __init__(self, X, y, 
                 cov_fn: Callable,
                 mean_fn: Optional[Callable] = None,
                 priors: Dict = None,
                 likelihood: AbstractLikelihood = None):  
        """Initialize the FullLatentGPModelRepeatedObs model.
        
        This is partially a repetition of the FullLatentGP init function, but 
        with some crucial differences; we store only the unique inputs, and the
        reverse indices to later repeat f back to the appropriate instances when
        we evaluate the likelihood.

        """
        if jnp.ndim(X) > 1:
            raise NotImplementedError(f'Repeated input models are only implemented for 1D input, ',
                                      f'but X is of shape {X.shape}')
        X = jnp.squeeze(X)
        # sort observations
        sort_idx = jnp.argsort(X, axis=0)
        X = X[sort_idx]
        y = y[sort_idx]

        # get unique values and reverse indices
        self.X, self.ix, self.rev_ix = jnp.unique(X, 
                                                  return_index=True, 
                                                  return_inverse=True)
        self.X = self.X[:, jnp.newaxis] 
        self.y = y

        if likelihood is None:
            likelihood = Gaussian()
        self.likelihood = RepeatedObsLikelihood(base_likelihood=likelihood,
                                                inv_i=self.rev_ix)  # not unique
        self.param_priors = priors
        if mean_fn is None:
            mean_fn = Zero()
        self.mean_fn = mean_fn
        self.cov_fn = cov_fn               

    #
    def forward(self, key, params, f):
        """Sample from the likelihood, given likelihood parameters and latent f.

        As we are now in 'prediction mode', we do not want to compute reverse
        indices for f.

        """
        return self.likelihood.likelihood(params, 
                                          f, 
                                          do_reverse=False).sample(seed=key)

    #
#
class FullMarginalGPModel(FullGPModel):
    """The marginal Gaussian process model.

    In case the likelihood of the GP is Gaussian, we marginalize out the latent
    GP f for (much) more efficient inference.

    The marginal Gaussian process model consists of observations (y), generated
    by a Gaussian observation model with hyperparameter sigma as input. The
    latent GP itself is parametrized by a mean function (mu) and a covariance
    function (cov). These can have optional hyperparameters (psi) and (theta).

    The generative model is given by:

    .. math::
        psi     &\sim p(\psi)\\
        theta   &\sim p(\theta) \\
        sigma   &\sim p(\sigma) \\
        y       &\sim N(mu, cov + \sigma^2 I_n)

    All scalar parameters are sampled using Gaussian random walk MCMC.

    """

    def __init__(self, X, y: Optional[Array] = None,
                 cov_fn: Optional[Callable] = None,
                 mean_fn: Callable = None,
                 priors: Dict = None,
                 **kwargs):
        self.likelihood = Gaussian()
        super().__init__(X, y, cov_fn, mean_fn, priors, **kwargs)        

    #
    
    def loglikelihood_fn(self) -> Callable:
        """Returns the log-likelihood function for the model given a state.

        Args:
            None

        Returns:
            A function that computes the log-likelihood of the model given a
            state.
        """
        jitter = 1e-6

        def loglikelihood_fn_(state: GibbsState) -> Float:
            position = getattr(state, 'position', state)
            psi = {param: position[param] for param in self.param_priors['mean']} if 'mean' in self.param_priors else {}
            psi = position.get('mean', {})
            theta = position['kernel']
            sigma = position['likelihood']['obs_noise']
            mean = self.mean_fn.mean(params=psi, x=self.X).flatten()
            cov = self.cov_fn.cross_covariance(params=theta,
                                               x=self.X,
                                               y=self.X) + (sigma ** 2 + jitter) * jnp.eye(self.X.shape[0])
            logprob = dx.MultivariateNormalFullCovariance(mean, cov).log_prob(self.y)
            return logprob

        #
        return loglikelihood_fn_

    #
    def predict_f(self, key: Array, x_pred: ArrayTree, num_subsample=-1):
        """Predict the latent f on unseen pointsand

        This function takes the approximated posterior (either by MCMC or SMC)
        and predicts new latent function evaluations f^*.

        Args:
            key: PRNGKey
            x_pred: x^*; the queried locations.
            num_subsample: By default, we return one predictive sample for each
            posterior sample. While accurate, this can be memory-intensive; this
            parameter can be used to thin the MC output to every n-th sample.

        Returns:
            f_samples: An array of samples of f^* from p(f^* | x^*, x, y)


        todo:
        - predict using either SMC or MCMC output
        - predict from prior if desired
        """
        if jnp.ndim(x_pred) == 1:
            x_pred = x_pred[:, jnp.newaxis]

        samples = self.get_monte_carlo_samples()
        flat_particles, _ = tree_flatten(samples)
        num_particles = flat_particles[0].shape[0]
        key_samples = jrnd.split(key, num_particles)

        mean_params = samples.get('mean', {})
        cov_params = samples['kernel']
        mean_params_in_axes = jax.tree_map(lambda l: 0, mean_params)
        cov_param_in_axes = jax.tree_map(lambda l: 0, cov_params)
        sample_fun = lambda key, mean_params_, cov_params_, obs_noise_: sample_predictive(key,
                                                                            mean_params=mean_params_,
                                                                            cov_params=cov_params_,
                                                                            mean_fn=self.mean_fn,
                                                                            cov_fn=self.cov_fn,
                                                                            x=self.X,
                                                                            z=x_pred,
                                                                            target=self.y,
                                                                            obs_noise=obs_noise_)
        keys = jrnd.split(key, num_particles)
        target_pred = jax.vmap(jax.jit(sample_fun),
                        in_axes=(0,
                                {k: 0 for k in mean_params},
                                cov_param_in_axes,
                                0))(keys,
                                        mean_params,
                                        cov_params,
                                        samples['likelihood']['obs_noise'])

        return target_pred
    
    def predict_f_particle(self, key, x_pred, particles, num_subsample=-1):
        """Predict the latent f on unseen pointsand

        This function takes the approximated posterior (either by MCMC or SMC)
        and predicts new latent function evaluations f^*.

        Args:
            key: PRNGKey
            x_pred: x^*; the queried locations.
            num_subsample: By default, we return one predictive sample for each
            posterior sample. While accurate, this can be memory-intensive; this
            parameter can be used to thin the MC output to every n-th sample.

        Returns:
            f_samples: An array of samples of f^* from p(f^* | x^*, x, y)


        todo:
        - predict using either SMC or MCMC output
        - predict from prior if desired
        """
        if jnp.ndim(x_pred) == 1:
            x_pred = x_pred[:, jnp.newaxis]

        samples = copy.deepcopy(particles)
        flat_particles, _ = tree_flatten(samples)
        num_particles = flat_particles[0].shape[0]
        key_samples = jrnd.split(key, num_particles)

        mean_params = samples.get('mean', {})
        cov_params = samples.get('kernel', {})
        mean_params_in_axes = jax.tree_map(lambda l: 0, mean_params)
        cov_param_in_axes = jax.tree_map(lambda l: 0, cov_params)
        sample_fun = lambda key, mean_params_, cov_params_, obs_noise_: sample_predictive(key,
                                                                            mean_params=mean_params_,
                                                                            cov_params=cov_params_,
                                                                            mean_fn=self.mean_fn,
                                                                            cov_fn=self.cov_fn,
                                                                            x=self.X,
                                                                            z=x_pred,
                                                                            target=self.y,
                                                                            obs_noise=obs_noise_)
        
        keys = jrnd.split(key, num_particles)
        
        key_subset1 = keys[:num_particles//2]
        key_subset2 = keys[num_particles//2:num_particles]
        key_subset_list = jnp.array([key_subset1, key_subset2])
        # print(key_subset_list.shape)
        if isinstance(cov_params, dict):
            if 'num' in cov_params.keys():            
                cov_params['num'] = jnp.append(cov_params['num'], jnp.full((cov_params['num'].shape[0], 1), jnp.nan), axis=1)
        else:
             for i, param in enumerate(cov_params):
                  if 'num' in cov_params[i].keys():            
                    cov_params[i]['num'] = jnp.append(cov_params[i]['num'], jnp.full((cov_params[i]['num'].shape[0], 1), jnp.nan), axis=1)
        
        cov_params_subset1 = jax.tree_map(lambda l: l[:num_particles//2], cov_params)
        cov_params_subset2 = jax.tree_map(lambda l: l[num_particles//2:num_particles], cov_params)
        cov_params_subset1_flat, subset1_tree_def = jax.tree_util.tree_flatten(cov_params_subset1)
        cov_params_subset2_flat, subset2_tree_def = jax.tree_util.tree_flatten(cov_params_subset2)
        
        
        cov_params_subset_array1 = jnp.zeros((1, cov_params_subset1_flat[0].shape[0], cov_params_subset1_flat[0].shape[1]))
        cov_params_subset_array2 = jnp.zeros((1, cov_params_subset2_flat[0].shape[0], cov_params_subset1_flat[0].shape[1]))
        for arr in cov_params_subset1_flat:
             cov_params_subset_array1 = jnp.concatenate((cov_params_subset_array1, jnp.expand_dims(arr, 0)))
            
        for arr in cov_params_subset2_flat:
             cov_params_subset_array2 = jnp.concatenate((cov_params_subset_array2, jnp.expand_dims(arr, 0)))

        cov_params_subset_array1_new = cov_params_subset_array1[1:, :, :]
        cov_params_subset_array2_new = cov_params_subset_array2[1:, :, :]

        cov_params_subset_array = jnp.array([cov_params_subset_array1_new, cov_params_subset_array2_new])

        lik_samples1 = samples['likelihood']['obs_noise'][:num_particles//2]
        lik_samples2 = samples['likelihood']['obs_noise'][num_particles//2:num_particles]
        lik_subset_list = jnp.array([lik_samples1, lik_samples2])

       
        def vmap_setup(key, m, cov, obs):
            new_cov = [x for x in cov]
            dict_cov = jax.tree_util.tree_unflatten(subset1_tree_def, new_cov)
            return jax.vmap(jax.jit(sample_fun), 
                              in_axes=(0, {k: 0 for k in mean_params}, cov_param_in_axes, 0))(key, m, dict_cov, obs)
        
        target_pred = jax.pmap(vmap_setup,
                                in_axes=(0,
                                        None,
                                        0,
                                        0), 
                                        devices = jax.devices())(key_subset_list,
                                                                    mean_params,
                                                                    cov_params_subset_array,
                                                                    lik_subset_list)
                

        return jnp.reshape(target_pred, (-1, len(x_pred)))
    #
    def predict_y(self, key: PRNGKey, x_pred: Array, particles = None):
        """Samples from the posterior predictive distribution

        Args:
            key: PRNGKey
            x_pred: Array
                The test locatons
        Returns:
            Returns samples from the posterior predictive distribution:

            y* \sim p(y* | X, y x*) = \int p(y* | f*)p(f* | f)p(f | X, y) df

        """
        if jnp.ndim(x_pred) == 1:
            x_pred = x_pred[:, jnp.newaxis]

        if particles is not None:
            samples = particles
        else:
            samples = self.get_monte_carlo_samples()

        if samples is None:
            raise AssertionError(
                f'The posterior predictive distribution can only be called after training.')

        def forward(key, params, f):
            return self.likelihood.likelihood(params, f).sample(seed=key)

        #
        key, key_f, key_y = jrnd.split(key, 3)
        if particles is not None:
            f_pred = self.predict_f_particle(key_f, x_pred, particles)
        else:
            f_pred = self.predict_f(key_f, x_pred)
        flat_particles, _ = tree_flatten(samples)
        num_particles = flat_particles[0].shape[0]
        keys_y = jrnd.split(key_y, num_particles)
        likelihood_params = samples['likelihood']
        y_pred = jax.vmap(jax.jit(forward),
                            in_axes=(0,
                                    0,
                                    0))(keys_y,
                                    likelihood_params,
                                    f_pred)
        return y_pred

class FullMarginalGPModelhyper_mult(FullMarginalGPModel):
    """The marginal Gaussian process model.

    In case the likelihood of the GP is Gaussian, we marginalize out the latent
    GP f for (much) more efficient inference.

    The marginal Gaussian process model consists of observations (y), generated
    by a Gaussian observation model with hyperparameter sigma as input. The
    latent GP itself is parametrized by a mean function (mu) and a covariance
    function (cov). These can have optional hyperparameters (psi) and (theta).

    The generative model is given by:

    .. math::
        psi     &\sim p(\psi)\\
        theta   &\sim p(\theta) \\
        sigma   &\sim p(\sigma) \\
        y       &\sim N(mu, cov + \sigma^2 I_n)

    All scalar parameters are sampled using Gaussian random walk MCMC.

    """

    def __init__(self, X, y: Optional[Array] = None,
                 cov_fn: Optional[Callable] = None,
                 mean_fn: Callable = None,
                 priors: Dict = None,
                 **kwargs):
        self.likelihood = Gaussian()
        super().__init__(X, y, cov_fn, mean_fn, priors, **kwargs)        

    #
    
    def init_fn(self, key: Array, num_particles: int = 1):
        """Initial state for MCMC/SMC.

        This function initializes all highest level latent variables. Children
        of this class need to implement initialization of intermediate latent
        variables according to the structure of the hierarchical model.

        Args:
            key: PRNGKey
            num_particles: int
                Number of particles to initialize a state for
        Returns:
            GibbsState

        """
        all_flat, all_priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, dict))
        OG_names = sorted(all_flat[0].keys())
        num_flat, num_priors_treedef = tree_flatten(self.param_priors['kernel'], lambda l: isinstance(l, Poisson_Process.Poisson_Process_hyper))
        num_distr = [obj for obj in num_flat if isinstance(obj, Poisson_Process.Poisson_Process_hyper)]
        
        priors_flat, priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, (Distribution, Bijector)))
        children = priors_treedef.children()
        leaves = [x.node_data()[1] for x in children]
       
        hyper_flat, hyper_priors_treedef = tree_flatten(self.param_priors['hyper'], lambda l: isinstance(l, (Distribution, Bijector)))

        samples = list()

        key, subkey = jrnd.split(key)


        if isinstance(self.param_priors['hyper'], dict):
            hyper_sample = hyper_flat[0].sample(seed=subkey, sample_shape=num_particles)
        else:
            hyper_sample = []
            for _, hyper_dist in enumerate(hyper_flat):
                key, subkey = jrnd.split(key)
                hyper_sample.append(hyper_dist.sample(seed=subkey, sample_shape=num_particles))       
        def horizon_check(x):
            return jnp.where(x>1, jnp.nan, x)
        if isinstance(self.param_priors['kernel'], dict):
            num_sample = jnp.sort(num_distr[0]._sample_n(subkey, num_particles, hyper_sample), axis = 1)
        else:
            num_sample=[]
            for i, num_s in enumerate(num_distr):
                key, subkey = jrnd.split(key)
                num_sample.append(jnp.sort(num_s._sample_n(subkey, num_particles, hyper_sample[i]), axis = 1))       

        for sup_name in OG_names:
            if sup_name == 'hyper':
                if isinstance(self.param_priors[sup_name], dict):
                    for name, prior in dict(sorted(self.param_priors[sup_name].items())).items():
                        samples.append(hyper_sample)
                else:
                    for i, sample in enumerate(hyper_sample):
                        samples.append(hyper_sample[i])

            if sup_name == 'likelihood':
                for name, prior in dict(sorted(self.param_priors[sup_name].items())).items():
                    samples.append(prior.sample(seed=subkey, sample_shape=num_particles))

            if sup_name == 'kernel':
                if isinstance(self.param_priors[sup_name], dict):
                    for name, prior in dict(sorted(self.param_priors[sup_name].items())).items():
                        if isinstance(prior, Poisson_Process.Poisson_Process_hyper):
                            samples.append(num_sample)
                        else:
                            key, subkey = jrnd.split(key)
                            samples.append(prior._sample_n(subkey, num_particles, num_sample))
                else:
                    for i, kernels in enumerate(self.param_priors[sup_name]):
                        for name, prior in dict(sorted(kernels.items())).items():
                            if isinstance(prior, Poisson_Process.Poisson_Process_hyper):
                                samples.append(num_sample[i])
                            else:
                                key, subkey = jrnd.split(key)
                                samples.append(prior._sample_n(subkey, num_particles, num_sample[i]))
        
        initial_position = jax.tree_util.tree_unflatten(priors_treedef, samples)
        return GibbsState(position=initial_position)


    
    #

    def logprior_fn(self) -> Callable:
        """Returns the log-prior function for the model given a state.

        This default logprior assumes a non-hierarchical model. If a 
        hierarchical model is used, the mode should implement its own 
        logprior_fn.

        Args:
            None
        Returns:
            A function that computes the log-prior of the model given a state.

        """

        def logprior_fn_(state: GibbsState):
            all_flat, orig_priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, dict))
            OG_names = sorted(all_flat[0].keys())

            position = getattr(state, 'position', state)
            logprob = 0
            
            num_flat, num_priors_treedef = tree_flatten(self.param_priors['kernel'], lambda l: isinstance(l, Poisson_Process.Poisson_Process_hyper))
            num_distr = [obj for obj in num_flat if isinstance(obj, Poisson_Process.Poisson_Process_hyper)]

            hyper_flat, hyper_priors_treedef = tree_flatten(self.param_priors['hyper'], lambda l: isinstance(l, (Distribution, Bijector)))         

            kernel_vals = position['kernel']
            hyper_vals = position['hyper']
            likelihood_vals = position['likelihood']

            if isinstance(hyper_vals, dict):
                hyper_val = list(hyper_vals.values())[0]
            else:
                hyper_val = []
                for hyper in hyper_vals:
                    hyper_val.append(list(hyper.values())[0])
            
            if isinstance(kernel_vals, dict):
                num_val = kernel_vals['num']
            else:
                num_val = []
                for num in kernel_vals:
                    num_val.append(num['num'])

            

            for sup_name in OG_names:
                if sup_name == 'kernel':
                    if isinstance(self.param_priors[sup_name], dict):
                        sorted_dict = dict(sorted(self.param_priors[sup_name].items()))
                        for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), kernel_vals.values()):
                            if isinstance(dist, Poisson_Process.Poisson_Process_hyper):
                                logprob += jnp.nansum(dist.log_prob(value, hyper_val)) \
                                    + jnp.nansum(hyper_flat[0].log_prob(hyper_val))
                            else:
                                logprob += jnp.nansum(dist.log_prob(value)) \
                                    + jnp.nansum(num_distr[0].log_prob(num_val, hyper_val)) 
                    else:
                        for i, kernels in enumerate(self.param_priors[sup_name]):
                            sorted_dict = dict(sorted(kernels.items()))
                            for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), kernel_vals[i].values()):
                                if isinstance(dist, Poisson_Process.Poisson_Process_hyper):
                                    logprob += jnp.nansum(dist.log_prob(value, hyper_val[i])) \
                                        + jnp.nansum(hyper_flat[i].log_prob(hyper_val[i]))
                                else:
                                    logprob += jnp.nansum(dist.log_prob(value)) \
                                        + jnp.nansum(num_distr[i].log_prob(num_val[i], hyper_val[i])) 
                elif sup_name == 'hyper':
                    if isinstance(self.param_priors[sup_name], dict):
                        sorted_dict = dict(sorted(self.param_priors[sup_name].items()))
                        for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), hyper_vals.values()):
                            logprob += jnp.sum(dist.log_prob(value))
                    else:
                        for i, hypers in enumerate(self.param_priors[sup_name]):
                            sorted_dict = dict(sorted(hypers.items()))
                            for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), hyper_vals[i].values()):
                                logprob += jnp.sum(dist.log_prob(value))
                elif sup_name == 'likelihood':
                    sorted_dict = dict(sorted(self.param_priors[sup_name].items()))
                    for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), likelihood_vals.values()):
                        logprob += jnp.sum(dist.log_prob(value))
    
            return logprob

        #
        return logprior_fn_

class FullMarginalGPModelhyper_mult_poisson(FullMarginalGPModelhyper_mult):
    def __init__(self, X, y: Optional[Array] = None,
                 cov_fn: Optional[Callable] = None,
                 mean_fn: Callable = None,
                 priors: Dict = None,
                 likelihood = None,
                 **kwargs):
        super().__init__(X, y, cov_fn, mean_fn, priors, **kwargs)        
        if likelihood is None:
            self.likelihood = Gaussian()
        else:
            self.likelihood = likelihood
        self.sampling_parameters = dict()
        
        sigma = 0.01
        priors_flat, _ = tree_flatten(self.param_priors, lambda l: isinstance(l, (Distribution, Bijector)))
        m = 0            
        for prior in priors_flat:
            m += jnp.prod(jnp.asarray(prior.batch_shape)) if prior.batch_shape else 1

        self.sampling_parameters['kernel'] = rmh
        self.sampling_parameters['kernel_parameters'] = dict(sigma=sigma*jnp.eye(m))


    def gibbs_fn(self, key: PRNGKey, state: GibbsState, temperature: Float= 1.0, **mcmc_parameters):
        key, key_poisson = jrnd.split(key, 2)
        
        def logdensity_(key, new_pos, old_pos, temperature):
                key, key_accept = jrnd.split(key, 2)
                delta = (temperature * loglikelihood_fn_(new_pos) + logprior_fn_(new_pos)) - \
                        (temperature * loglikelihood_fn_(old_pos) + logprior_fn_(old_pos))
                delta = jnp.where(jnp.isnan(delta), -jnp.inf, delta)
                p_accept = jnp.clip(jnp.exp(delta), a_max=1.0)
                do_accept = jax.random.bernoulli(key_accept, p_accept)
                
                return jax.lax.cond(
                    do_accept, lambda _: new_pos, lambda _: old_pos, operand=None
                )
        
        def poisson_process_step(key, position, temperature):

            new_position = position.copy()

            def nothing(key, params, index):
                return params
            # if p: # add value
            def add_param(key, params, index):
                key, sample_key = jrnd.split(key)
                new_val = jrnd.uniform(sample_key)
                new_num_params = params.at[-1].set(new_val)
                
                return new_num_params

            def add_num(key, params):
                index = jnp.nanargmax(params).astype(int)
                new_num_params = jax.lax.cond(index != (params.shape[0]-1), add_param, nothing, key, params, index)
                new_index = jnp.argmax(jnp.argsort(new_num_params))
                new_num_params = jnp.sort(new_num_params)
                return new_num_params, new_index
            
            def remove_param(key, params, index):
                new_num_params = params.at[index].set(jnp.nan)
                return new_num_params
            
            def remove_num(key, params):
                index = jnp.rint(jrnd.uniform(key)*jnp.nanargmax(params)).astype(int)
                new_num_params = jax.lax.cond(jnp.nanargmax(params) != -1, remove_param, nothing, key, params, index)
                return new_num_params, index

            def new_nothing(key, value, index, dist):
                return value

            def swap_param(i, value):
                indices = jnp.array([value.shape[0]-1-(i+1), value.shape[0]-1-(i)])
                swap_vals = jnp.array([value[value.shape[0]-1-(i)], value[value.shape[0]-1-(i+1)]])
                value = value.at[indices].set(swap_vals)
                return value

            def add_cov_param(key, value, index, dist):
                
                key, sample_key = jrnd.split(key)
                k = jnp.zeros((1, value.shape[0]))
                k = k.at[:].set(jnp.nan)
                k = k.at[0].set(0)
                value = value.at[-1].set(dist._sample_n(sample_key, 1, k)[0])
                
                new_value = jax.lax.fori_loop(0, value.shape[0]-1-(index+1), swap_param, value)
                return new_value

            def add_cov(key, value, index, dist):
                value = remove_cov_param(key, value, index-1)
                value = jax.lax.cond(index != len(num_params), add_cov_param, new_nothing, key, value, index-1, dist)
                value = jax.lax.cond(index != len(num_params), add_cov_param, new_nothing, key, value, index, dist)
                return value
            
            def swap_nan(i, value):
                indices = jnp.array([i, i+1])
                swap_vals = jax.lax.cond(jnp.isnan(value[i]) & ~jnp.isnan(value[i+1]), 
                                            lambda _: jnp.array([value[i+1], value[i]]),
                                            lambda _: jnp.array([value[i], value[i+1]]), 
                                            operand=None)
                value = value.at[indices].set(swap_vals)
                return value

            def remove_cov_param(key, value, index):
                value = value.at[index+1].set(jnp.nan)
                
                new_value = jax.lax.fori_loop(0, value.shape[0]-2, swap_nan, value)
                return new_value

            def remove_cov(key, value, index, dist):
                value = jax.lax.cond(index != -1, remove_cov_param, nothing, key, value, index)
                value = jax.lax.cond(index != -1, remove_cov_param, nothing, key, value, index-1)
                value = add_cov_param(key, value, index-1, dist)
                    
                return value
            
            priors_flat, priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, (Distribution, Bijector)))
            
            if isinstance(new_position['kernel'], dict):
                key, key_poisson = jrnd.split(key)
                p = jrnd.bernoulli(key=key_poisson)   
                num_params = jnp.sort(new_position['kernel']['num'])
                new_num_params, index = jax.lax.cond(p, add_num, remove_num, key, num_params)
                new_position['kernel']['num'] = new_num_params
                ## covariance parameters
                cov_params = copy.deepcopy(position['kernel'])
                del cov_params['num'] 
                cov_priors = copy.deepcopy(self.param_priors['kernel'])
                del cov_priors['num'] 
                priors_flat, priors_treedef = tree_flatten(cov_priors, lambda l: isinstance(l, (Distribution, Bijector)))
                values_flat, _ = tree_flatten(cov_params)
                names = [x for x in priors_treedef.node_data()[1]]
                
                for value, dist, name in zip(values_flat, priors_flat, names):
                    value = jax.lax.cond(p, add_cov, remove_cov, key, value, index, dist)
                    new_position['kernel'][name] = value
            else:
                for i, kernels in enumerate(new_position['kernel']):
                    key, key_poisson = jrnd.split(key)
                    p = jrnd.bernoulli(key=key_poisson)   
                    num_params = jnp.sort(kernels['num'])
                    new_num_params, index = jax.lax.cond(p, add_num, remove_num, key, num_params)
                    kernels['num'] = new_num_params
                    ## covariance parameters
                    cov_params = copy.deepcopy(kernels)
                    del cov_params['num'] 
                    cov_priors = copy.deepcopy(self.param_priors['kernel'][i])
                    del cov_priors['num'] 
                    priors_flat, priors_treedef = tree_flatten(cov_priors, lambda l: isinstance(l, (Distribution, Bijector)))
                    values_flat, _ = tree_flatten(cov_params)
                    names = [x for x in priors_treedef.node_data()[1]]
                
                    for value, dist, name in zip(values_flat, priors_flat, names):
                        value = jax.lax.cond(p, add_cov, remove_cov, key, value, index, dist)
                        new_position['kernel'][i][name] = value
            
            return new_position
            
        position = state.position.copy()
        loglikelihood_fn_ = self.loglikelihood_fn()
        logprior_fn_ = self.logprior_fn()
        logdensity = lambda state: temperature * loglikelihood_fn_(state) + logprior_fn_(state)

        ptree, unravel_fn = ravel_pytree(position)
        sample = jnp.zeros(shape=ptree.shape, dtype=ptree.dtype)
        move_proposal = unravel_fn(sample)
        old_position = jax.tree_util.tree_map(jnp.add, position, move_proposal)

        new_pos = poisson_process_step(key_poisson, position, temperature)
        poisson_pos = logdensity_(key, new_pos, old_position, temperature)

        def apply_mcmc_kernel(key, logdensity, pos):
                        kernel = kernel_type(logdensity, **kernel_parameters)
                        state_ = kernel.init(pos)
                        state_, info = kernel.step(key, state_)
                        return state_.position, info

        kernel_type = self.sampling_parameters.get('kernel')
        kernel_parameters = self.sampling_parameters.get('kernel_parameters')
        new_position, info_ = apply_mcmc_kernel(key, logdensity, poisson_pos)

        return GibbsState(position=new_position), None

class FullLatentGPModelhyper_mult(FullLatentGPModel):
    """The latent Gaussian process model.

        The latent Gaussian process model consists of observations (y), generated by
        an observation model that takes the latent Gaussian process (f) and optional
        hyperparameters (phi) as input. The latent GP itself is parametrized by a
        mean function (mu) and a covariance function (cov). These can have optional
        hyperparameters (psi) and (theta).

        The generative model is given by:

        .. math::
            psi     &\sim p(\psi)\\
            theta   &\sim p(\theta) \\
            phi     &\sim p(\phi) \\
            f       &\sim GP(mu, cov) \\
            y       &\sim p(y \mid T(f), phi)

        Here, the scalar parameters are sampled using Gaussian random walk MCMC, 
        while the latent function f (or rather its evaluations) is sampled using
        Elliptical Slice Sampling.

    """

    def __init__(self, X, y: Optional[Array] = None,
                 cov_fn: Optional[Callable] = None,
                 mean_fn: Callable = None,
                 priors: Dict = None,
                 likelihood = None,
                 **kwargs):
        if likelihood is None:
            self.likelihood = Gaussian()
        else:
            self.likelihood = likelihood
        super().__init__(X, y, cov_fn, mean_fn, priors, **kwargs)        

    #
    
    def init_fn(self, key, num_particles=1):
        """Initialization of the Gibbs state.

        The initial state is determined by sampling all variables from their
        priors, and then constructing one sample for (f) using these. All
        variables together are stored in a dict() in the GibbsState object.

        When num_particles > 1, each dict element contains num_particles random
        initial values.

        Args:
            key:
                The jax.random.PRNGKey
            num_particles: int
                The number of parallel particles for sequential Monte Carlo
        Returns:
            GibbsState

        """
        all_flat, all_priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, dict))
        OG_names = sorted(all_flat[0].keys())
        num_flat, num_priors_treedef = tree_flatten(self.param_priors['kernel'], lambda l: isinstance(l, Poisson_Process.Poisson_Process_hyper))
        num_distr = [obj for obj in num_flat if isinstance(obj, Poisson_Process.Poisson_Process_hyper)]
        
        priors_flat, priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, (Distribution, Bijector)))
        children = priors_treedef.children()
        leaves = [x.node_data()[1] for x in children]
        
        hyper_flat, hyper_priors_treedef = tree_flatten(self.param_priors['hyper'], lambda l: isinstance(l, (Distribution, Bijector)))
       
        samples = list()
        key, subkey = jrnd.split(key)

        if isinstance(self.param_priors['hyper'], dict):
            hyper_sample = hyper_flat[0].sample(seed=subkey, sample_shape=num_particles)
        else:
            hyper_sample = []
            for _, hyper_dist in enumerate(hyper_flat):
                key, subkey = jrnd.split(key)
                hyper_sample.append(hyper_dist.sample(seed=subkey, sample_shape=num_particles))       
        
        if isinstance(self.param_priors['kernel'], dict):
            num_sample = jnp.sort(num_distr[0]._sample_n(subkey, num_particles, hyper_sample), axis = 1)
        else:
            num_sample= []
            for i, num_s in enumerate(num_distr):
                num_sample.append(jnp.sort(num_s._sample_n(subkey, num_particles, hyper_sample[i]), axis = 1))       

        for sup_name in OG_names:
            if sup_name == 'hyper':
                if isinstance(self.param_priors[sup_name], dict):
                    for name, prior in dict(sorted(self.param_priors[sup_name].items())).items():
                        samples.append(hyper_sample)
                else:
                    for i, sample in enumerate(hyper_sample):
                        samples.append(hyper_sample[i])

            if sup_name == 'likelihood':
                for name, prior in dict(sorted(self.param_priors[sup_name].items())).items():
                    samples.append(prior.sample(seed=subkey, sample_shape=num_particles))

            if sup_name == 'kernel':
                if isinstance(self.param_priors[sup_name], dict):
                    for name, prior in dict(sorted(self.param_priors[sup_name].items())).items():
                        if isinstance(prior, Poisson_Process.Poisson_Process_hyper):
                            samples.append(num_sample)
                        else:
                            key, subkey = jrnd.split(key)
                            samples.append(prior._sample_n(subkey, num_particles, num_sample,))
                else:
                    for i, kernels in enumerate(self.param_priors[sup_name]):
                        for name, prior in dict(sorted(kernels.items())).items():
                            if isinstance(prior, Poisson_Process.Poisson_Process_hyper):
                                samples.append(num_sample[i])
                            else:
                                key, subkey = jrnd.split(key)
                                samples.append(prior._sample_n(subkey, num_particles, num_sample[i]))
        
        initial_position = jax.tree_util.tree_unflatten(priors_treedef, samples)

        mean_params = initial_position.get('mean', {})
        cov_params = initial_position.get('kernel', {})
        mean_param_in_axes = jax.tree_map(lambda l: 0, mean_params)
        cov_param_in_axes = jax.tree_map(lambda l: 0, cov_params)

        if num_particles > 1:
            keys = jrnd.split(key, num_particles)                
            sample_fun = lambda key_, mean_params_, cov_params_: sample_prior(key=key_, 
                                                                                mean_params=mean_params_,
                                                                                cov_params=cov_params_,
                                                                                mean_fn=self.mean_fn,
                                                                                cov_fn=self.cov_fn, 
                                                                                x=self.X)
            initial_position['f'] = jax.vmap(sample_fun,
                                             in_axes=(0,
                                                      mean_param_in_axes,
                                                      cov_param_in_axes))(keys, mean_params, cov_params)
        else:
            key, subkey = jrnd.split(key)
            initial_position['f'] = sample_prior(subkey, 
                                                 self.mean_fn, 
                                                 self.cov_fn, 
                                                 mean_params, 
                                                 cov_params, 
                                                 self.X)
        return GibbsState(initial_position)
    

    def loglikelihood_fn_new(self, position, temperature) -> Float:
        
        f = position['f']
        
        X = self.X
        mu = Zero().mean(params=None, x=X)
        n = X.shape[0]

        cov_params = position.get('kernel', {}) 
        jitter =  1e-5
       
        temp = 0
        cov_ = self.cov_fn.cross_covariance(params=cov_params, x=X, y=X) + jitter * jnp.eye(n)
        log_pdf = dx.MultivariateNormalFullCovariance(mu, cov_).log_prob(f)
        
        return log_pdf
        
            
    def logprior_fn_new(self, position):

        all_flat, orig_priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, dict))

        OG_names = sorted(all_flat[0].keys())

        logprob = 0
        
        num_flat, num_priors_treedef = tree_flatten(self.param_priors['kernel'], lambda l: isinstance(l, Poisson_Process.Poisson_Process_hyper))
        num_distr = [obj for obj in num_flat if isinstance(obj, Poisson_Process.Poisson_Process_hyper)]
        hyper_flat, hyper_priors_treedef = tree_flatten(self.param_priors['hyper'], lambda l: isinstance(l, (Distribution, Bijector)))
        kernel_vals = position.get('kernel', {}) 
        hyper_vals = position.get('hyper', {}) 
        likelihood_vals = position.get('likelihood', {}) 

        if isinstance(hyper_vals, dict):
            hyper_val = list(hyper_vals.values())[0]
        else:
            hyper_val = []
            for hyper in hyper_vals:
                hyper_val.append(list(hyper.values())[0])
        
        if isinstance(kernel_vals, dict):
            num_val = kernel_vals['num']
        else:
            num_val = []
            for num in kernel_vals:
                num_val.append(num['num'])

        

        for sup_name in OG_names:
            if sup_name == 'kernel':
                if isinstance(self.param_priors[sup_name], dict):
                    sorted_dict = dict(sorted(self.param_priors[sup_name].items()))
                    for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), kernel_vals.values()):
                        if isinstance(dist, Poisson_Process.Poisson_Process_hyper):
                            logprob += jnp.nansum(dist.log_prob(value, hyper_val)) \
                                + jnp.nansum(hyper_flat[0].log_prob(hyper_val))
                        else:
                            logprob += jnp.nansum(dist.log_prob(value)) \
                                + jnp.nansum(num_distr[0].log_prob(num_val, hyper_val)) 
                else:
                    for i, kernels in enumerate(self.param_priors[sup_name]):
                        sorted_dict = dict(sorted(kernels.items()))
                        for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), kernel_vals[i].values()):
                            if isinstance(dist, Poisson_Process.Poisson_Process_hyper):
                                logprob += jnp.nansum(dist.log_prob(value, hyper_val[i])) \
                                    + jnp.nansum(hyper_flat[i].log_prob(hyper_val[i]))
                            else:
                                logprob += jnp.nansum(dist.log_prob(value)) \
                                    + jnp.nansum(num_distr[i].log_prob(num_val[i], hyper_val[i])) 
            elif sup_name == 'hyper':
                if isinstance(self.param_priors[sup_name], dict):
                    sorted_dict = dict(sorted(self.param_priors[sup_name].items()))
                    for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), hyper_vals.values()):
                        logprob += jnp.sum(dist.log_prob(value))
                else:
                    for i, hypers in enumerate(self.param_priors[sup_name]):
                        sorted_dict = dict(sorted(hypers.items()))
                        for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), hyper_vals[i].values()):
                            logprob += jnp.sum(dist.log_prob(value))
            elif sup_name == 'likelihood':
                sorted_dict = dict(sorted(self.param_priors[sup_name].items()))
                for name, dist, value in zip(sorted_dict.keys(), sorted_dict.values(), likelihood_vals.values()):
                    logprob += jnp.sum(dist.log_prob(value))
        
        return logprob

    #
    def gibbs_fn(self, key: PRNGKey, state: GibbsState, temperature: Float= 1.0, poisson = True, **mcmc_parameters):
        """The Gibbs MCMC kernel.

        The Gibbs kernel step function takes a state and returns a new state. In
        the latent GP model, the latent GP (f) is first updated, then the
        parameters of the mean (psi) and covariance function (theta), and lastly
        the parameters of the observation model (phi).

        Args:
            key:
                The jax.random.PRNGKey
            state: GibbsState
                The current state in the MCMC sampler
            temperature: Float
                The likeihood temperature, \beta in p_\beta(x | y) \propto p(x) p(y | x)^\beta
            mcmc_parameters: Dict
                A dictionary with optional settings for the MCMC-within-Gibbs 
                steps. TODO
        Returns:
            GibbsState

        """

        ## do a poisson process update:
        key, key_poisson = jrnd.split(key, 2)
                    
        p = jrnd.bernoulli(key=key_poisson)  

        def logdensity_(key, new_pos, old_pos, temperature):
                key, key_accept = jrnd.split(key, 2)
                delta = (temperature * self.loglikelihood_fn_new(new_pos, temperature) + self.logprior_fn_new(new_pos)) - \
                        (temperature * self.loglikelihood_fn_new(old_pos, temperature) + self.logprior_fn_new(old_pos))
                delta = jnp.where(jnp.isnan(delta), -jnp.inf, delta)
                p_accept = jnp.clip(jnp.exp(delta), a_max=1.0)
                do_accept = jax.random.bernoulli(key_accept, p_accept)
                

                return jax.lax.cond(
                    do_accept, lambda _: new_pos, lambda _: old_pos, operand=None
                )

        def poisson_process_step_new(key, position, temperature):
            new_position = position.copy()

            def nothing(key, params, index):
                return params
            # if p: # add value
            def add_param(key, params, index):
                key, sample_key = jrnd.split(key)
                new_val = jrnd.uniform(sample_key)
                new_num_params = params.at[-1].set(new_val)
                
                return new_num_params

            def add_num(key, params):
                index = jnp.nanargmax(params).astype(int)
                new_num_params = jax.lax.cond(index != (params.shape[0]-1), add_param, nothing, key, params, index)
                new_index = jnp.argmax(jnp.argsort(new_num_params))
                new_num_params = jnp.sort(new_num_params)
                return new_num_params, new_index
            
            def remove_param(key, params, index):
                new_num_params = params.at[index].set(jnp.nan)
                return new_num_params
            
            def remove_num(key, params):
                index = jnp.rint(jrnd.uniform(key)*jnp.nanargmax(params)).astype(int)
                new_num_params = jax.lax.cond(jnp.nanargmax(params) != -1, remove_param, nothing, key, params, index)
                return new_num_params, index

            def new_nothing(key, value, index, dist):
                return value

            def swap_param(i, value):
                indices = jnp.array([value.shape[0]-1-(i+1), value.shape[0]-1-(i)])
                swap_vals = jnp.array([value[value.shape[0]-1-(i)], value[value.shape[0]-1-(i+1)]])
                value = value.at[indices].set(swap_vals)
                return value

            def add_cov_param(key, value, index, dist):
                
                key, sample_key = jrnd.split(key)
                k = jnp.zeros((1, value.shape[0]))
                k = k.at[:].set(jnp.nan)
                k = k.at[0].set(0)
                value = value.at[-1].set(dist._sample_n(sample_key, 1, k)[0])
                
                new_value = jax.lax.fori_loop(0, value.shape[0]-1-(index+1), swap_param, value)
                return new_value

            def add_cov(key, value, index, dist):
                value = remove_cov_param(key, value, index-1)
                value = jax.lax.cond(index != len(num_params), add_cov_param, new_nothing, key, value, index-1, dist)
                value = jax.lax.cond(index != len(num_params), add_cov_param, new_nothing, key, value, index, dist)
                return value
            
            def swap_nan(i, value):
                indices = jnp.array([i, i+1])
                swap_vals = jax.lax.cond(jnp.isnan(value[i]) & ~jnp.isnan(value[i+1]), 
                                            lambda _: jnp.array([value[i+1], value[i]]),
                                            lambda _: jnp.array([value[i], value[i+1]]), 
                                            operand=None)
                value = value.at[indices].set(swap_vals)
                return value

            def remove_cov_param(key, value, index):
                value = value.at[index+1].set(jnp.nan)
                
                new_value = jax.lax.fori_loop(0, value.shape[0]-2, swap_nan, value)
                return new_value

            def remove_cov(key, value, index, dist):
                value = jax.lax.cond(index != -1, remove_cov_param, nothing, key, value, index)
                value = jax.lax.cond(index != -1, remove_cov_param, nothing, key, value, index-1)
                value = add_cov_param(key, value, index-1, dist)
                    
                return value
            
            priors_flat, priors_treedef = tree_flatten(self.param_priors, lambda l: isinstance(l, (Distribution, Bijector)))
           
            if isinstance(new_position['kernel'], dict):
                key, key_poisson = jrnd.split(key)
                p = jrnd.bernoulli(key=key_poisson)   
                num_params = jnp.sort(new_position['kernel']['num'])
                new_num_params, index = jax.lax.cond(p, add_num, remove_num, key, num_params)
                new_position['kernel']['num'] = new_num_params
                ## covariance parameters
                cov_params = copy.deepcopy(position['kernel'])
                del cov_params['num'] 
                cov_priors = copy.deepcopy(self.param_priors['kernel'])
                del cov_priors['num'] 
                priors_flat, priors_treedef = tree_flatten(cov_priors, lambda l: isinstance(l, (Distribution, Bijector)))
                values_flat, _ = tree_flatten(cov_params)
                names = [x for x in priors_treedef.node_data()[1]]
                
                for value, dist, name in zip(values_flat, priors_flat, names):
                    value = jax.lax.cond(p, add_cov, remove_cov, key, value, index, dist)
                    new_position['kernel'][name] = value
            else:
                for i, kernels in enumerate(new_position['kernel']):
                    key, key_poisson = jrnd.split(key)
                    p = jrnd.bernoulli(key=key_poisson)   
                    num_params = jnp.sort(kernels['num'])
                    new_num_params, index = jax.lax.cond(p, add_num, remove_num, key, num_params)
                    kernels['num'] = new_num_params
                    ## covariance parameters
                    cov_params = copy.deepcopy(kernels)
                    del cov_params['num'] 
                    cov_priors = copy.deepcopy(self.param_priors['kernel'][i])
                    del cov_priors['num'] 
                    priors_flat, priors_treedef = tree_flatten(cov_priors, lambda l: isinstance(l, (Distribution, Bijector)))
                    values_flat, _ = tree_flatten(cov_params)
                    names = [x for x in priors_treedef.node_data()[1]]
                
                    for value, dist, name in zip(values_flat, priors_flat, names):
                        value = jax.lax.cond(p, add_cov, remove_cov, key, value, index, dist)
                        new_position['kernel'][i][name] = value
            
            
            return new_position
            
        position = getattr(state, 'position', state)
 

        ptree, unravel_fn = ravel_pytree(position)
        sample = jnp.zeros(shape=ptree.shape, dtype=ptree.dtype)
        move_proposal = unravel_fn(sample)
        old_position = jax.tree_util.tree_map(jnp.add, position, move_proposal)
 

        if poisson:
            new_pos = poisson_process_step_new(key_poisson, position, temperature)
            position = logdensity_(key, new_pos, old_position, temperature)
        

        # Sample the latent GP using:   
        # p(f | theta, psi, y) \propto p(y | f, phi) p(f | psi, theta)

        likelihood_params = position.get('likelihood', {})
        mean_params = position.get('mean', {}) 
        cov_params = position.get('kernel', {}) 
        hyper_params = position.get('hyper', {}) 

        loglikelihood_fn_ = lambda f_: temperature * jnp.sum(self.likelihood.log_prob(params=likelihood_params, f=f_, y=self.y))

        key, subkey = jrnd.split(key)
        position['f'], f_info = update_gaussian_process(subkey,
                                                        position['f'],
                                                        loglikelihood_fn_,
                                                        self.X,
                                                        mean_fn=self.mean_fn,
                                                        cov_fn=self.cov_fn,
                                                        mean_params=mean_params,
                                                        cov_params=cov_params)
        
        if len(hyper_params):
            if isinstance(self.param_priors['hyper'], dict):
                key, subkey = jrnd.split(key)
                sub_state, sub_info = update_gaussian_process_hyper_params(subkey, self.X,
                                        num_params = cov_params['num'],
                                        num_priors = self.param_priors['kernel']['num'], 
                                        hyper_params = hyper_params,
                                        hyperpriors=self.param_priors['hyper'])
                position['hyper'] = sub_state
                hyper_params = position.get('hyper', {}) 
            else: 
                for i, hyper in enumerate(self.param_priors['hyper']):
                    hyper_params = position.get('hyper', {}) [i]
                    cov_params = position.get('kernel', {})[i]
                    key, subkey = jrnd.split(key)
                    sub_state, sub_info = update_gaussian_process_hyper_params(subkey, self.X,
                                            num_params = cov_params['num'],
                                            num_priors = self.param_priors['kernel'][i]['num'], 
                                            hyper_params = hyper_params,
                                            hyperpriors=hyper)
                    position['hyper'][i] = sub_state
                    hyper_params = position.get('hyper', {}) 

        if len(mean_params):
            # Sample parameters of the mean function using: 
            # p(psi | f, theta) \propto p(f | psi, theta)p(psi)      

            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_gaussian_process_mean_params(subkey, self.X,
                                       position['f'],
                                       mean_fn=self.mean_fn,
                                       cov_fn=self.cov_fn,
                                       mean_params=mean_params,
                                       cov_params=cov_params,
                                       hyperpriors=self.param_priors['mean'])
            position['mean'] = sub_state

        
        if len(cov_params):
            # Sample parameters of the kernel function using: 
            # p(theta | f, psi) \propto p(f | psi, theta)p(theta)
            
            if isinstance(self.param_priors['kernel'], dict):
            
                key, subkey = jrnd.split(key)
                sub_state, sub_info = update_gaussian_process_cov_params_num_hyper(subkey, self.X,
                                        position['f'],
                                        mean_fn=self.mean_fn,
                                        cov_fn=self.cov_fn,
                                        mean_params=mean_params,
                                        cov_params=cov_params,
                                        hyper_params = hyper_params,
                                        hyperpriors=self.param_priors['kernel'], 
                                        temp=temperature,
                                        )
                
                position['kernel']['num'] = sub_state['num']

                cov_params = position.get('kernel', {}) 

                sub_state, sub_info = update_gaussian_process_cov_params_lv_hyper(subkey, self.X,
                                        position['f'],
                                        mean_fn=self.mean_fn,
                                        cov_fn=self.cov_fn,
                                        mean_params=mean_params,
                                        cov_params=cov_params,
                                        hyper_params = hyper_params,
                                        hyperpriors=self.param_priors['kernel'],
                                        temp=temperature,
                                        )
                
                
                sub_state['num'] = cov_params['num']
                position['kernel'] = sub_state
                
            else:
                for i, (kernel, kernel_val) in enumerate(zip(self.param_priors['kernel'], position['kernel'])):
                    if not isinstance(position['kernel'][i], dict):
                        raise ValueError(f'type is {position}')
                    hyper_params = position['hyper'][i]
                    cov_params = position['kernel'][i]
                    key, subkey = jrnd.split(key)
                    
                    sub_state, sub_info = update_gaussian_process_cov_params_num_hyper(subkey, self.X,
                                            position['f'],
                                            mean_fn=self.mean_fn,
                                            cov_fn=self.cov_fn,
                                            mean_params=mean_params,
                                            cov_params=position['kernel'],
                                            hyper_params = hyper_params,
                                            hyperpriors=self.param_priors['kernel'], 
                                            temp=temperature, 
                                            kernel_index = i,
                                            )
                    
                    position['kernel'][i]['num'] = sub_state['num']

                    cov_params = position['kernel'][i]

                    sub_state, sub_info = update_gaussian_process_cov_params_lv_hyper(subkey, self.X,
                                            position['f'],
                                            mean_fn=self.mean_fn,
                                            cov_fn=self.cov_fn,
                                            mean_params=mean_params,
                                            cov_params=position['kernel'],
                                            hyper_params = hyper_params,
                                            hyperpriors=self.param_priors['kernel'],
                                            temp=temperature,
                                            kernel_index = i,
                                            )
                    
                    
                    sub_state['num'] = cov_params['num']
                    position['kernel'][i] = sub_state
    
        if len(likelihood_params):
            # Sample parameters of the likelihood using: 
            # p(\phi | y, f) \propto p(y | f, phi)p(phi)

            key, subkey = jrnd.split(key)
            sub_state, sub_info = update_gaussian_process_obs_params(subkey, self.y,
                                       position['f'],
                                       temperature=temperature,
                                       likelihood=self.likelihood,
                                       obs_params=likelihood_params,
                                       hyperpriors=self.param_priors['likelihood'])
            position['likelihood'] = sub_state
        
        #
        return GibbsState(position=position), None  # We return None to satisfy SMC; this needs to be filled with acceptance information

            

#
